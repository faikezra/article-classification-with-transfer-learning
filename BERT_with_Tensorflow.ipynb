{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT with Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "654dc6eb422087a85df50aad9646e3c3b8692e5d",
        "id": "OnJj2VCJokxE",
        "colab_type": "text"
      },
      "source": [
        "#                                                                                 BERT\n",
        "\n",
        "BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\n",
        "\n",
        "Academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: https://arxiv.org/abs/1810.04805.\n",
        "\n",
        "Github account for the paper can be found here: https://github.com/google-research/bert\n",
        "\n",
        "BERT is a method of pre-training language representations. It is a model that has been pre-trained on a very large dataset. The unsupervised, deeply bidirectional system for pre-training NLP makes it possible for this encoder to be transfered to various downstream NLP tasks. Like our news classification problem..\n",
        "\n",
        "The notebook can simply be run on Google Colab due to heavy (10 - 16GB) GPU requirements. TPU acess is also available through Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8d6fe072db4c37b40476a41be674be47312d6251",
        "id": "GPbudyjJokxG",
        "colab_type": "text"
      },
      "source": [
        "![](https://www.lyrn.ai/wp-content/uploads/2018/11/transformer.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNJPehb-SlgB",
        "colab_type": "code",
        "outputId": "643af6bb-1a25-4f0f-a56e-1f380fcb8f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# If you are on Colab you can run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ASrAuzjSokxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import sys\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "04NdGYsookxM",
        "colab_type": "code",
        "outputId": "27f8027a-f124-4e58-b494-1670ca19a350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#downloading weights and cofiguration file for the model\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-06 12:59:09--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 2607:f8b0:4001:c03::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip.3’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   217MB/s    in 1.8s    \n",
            "\n",
            "2019-07-06 12:59:11 (217 MB/s) - ‘uncased_L-12_H-768_A-12.zip.3’ saved [407727028/407727028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8cf1f5466e25bbf042e0b6d55355d0bf7f02984e",
        "id": "s97PmDZbokxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "repo = 'model_repo'\n",
        "with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(repo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b4a8b81c1fb8b5fae20945a56fda981550c54342",
        "id": "stICkGGtokxV",
        "colab_type": "code",
        "outputId": "1c862fab-33ef-4f70-9c65-2367a869f067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# contents of the repo;\n",
        "# config file for model configurations\n",
        "# checkpoint (pretraining information)\n",
        "!ls 'model_repo/uncased_L-12_H-768_A-12'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_config.json\t\t     bert_model.ckpt.index  vocab.txt\n",
            "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0595c54d1033b9b0e69c5e565a8063a32295c3ff",
        "id": "aDfkDv8fokxZ",
        "colab_type": "code",
        "outputId": "ece29c17-2997-49ce-ff3d-0c0c6c68a543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-06 12:59:18--  https://raw.githubusercontent.com/google-research/bert/master/modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37922 (37K) [text/plain]\n",
            "Saving to: ‘modeling.py.3’\n",
            "\n",
            "\rmodeling.py.3         0%[                    ]       0  --.-KB/s               \rmodeling.py.3       100%[===================>]  37.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-07-06 12:59:18 (3.65 MB/s) - ‘modeling.py.3’ saved [37922/37922]\n",
            "\n",
            "--2019-07-06 12:59:20--  https://raw.githubusercontent.com/google-research/bert/master/optimization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6258 (6.1K) [text/plain]\n",
            "Saving to: ‘optimization.py.3’\n",
            "\n",
            "optimization.py.3   100%[===================>]   6.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-07-06 12:59:20 (106 MB/s) - ‘optimization.py.3’ saved [6258/6258]\n",
            "\n",
            "--2019-07-06 12:59:22--  https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34783 (34K) [text/plain]\n",
            "Saving to: ‘run_classifier.py.3’\n",
            "\n",
            "run_classifier.py.3 100%[===================>]  33.97K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-07-06 12:59:22 (3.22 MB/s) - ‘run_classifier.py.3’ saved [34783/34783]\n",
            "\n",
            "--2019-07-06 12:59:23--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12257 (12K) [text/plain]\n",
            "Saving to: ‘tokenization.py.3’\n",
            "\n",
            "tokenization.py.3   100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-07-06 12:59:23 (142 MB/s) - ‘tokenization.py.3’ saved [12257/12257]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "68f9932a52af968d1e94d0e115fc06e3423e1d47",
        "id": "u4Jqc1-xokxe",
        "colab_type": "text"
      },
      "source": [
        "Example below is done on preprocessing code, similar to **CoLa**:\n",
        "\n",
        "The Corpus of Linguistic Acceptability is\n",
        "a binary single-sentence classification task, where \n",
        "the goal is to predict whether an English sentence\n",
        "is linguistically “acceptable” or not\n",
        "\n",
        "You can use pretrained BERT model for wide variety of tasks, including classification.\n",
        "The task of CoLa is close to the task of Quora competition, so I thought it woud be interesting to use that example.\n",
        "Obviously, outside sources aren't allowed in Quora competition, so you won't be able to use BERT to submit a prediction.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0f47047973d523029fbe9c355577133c4656ce57",
        "id": "xTn1zCBsokxf",
        "colab_type": "code",
        "outputId": "bfe4d92c-ab8c-4699-fa34-6b9204a1b8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "#We will use the most basic of all of them\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
        "BERT_PRETRAINED_DIR = f'{repo}/uncased_L-12_H-768_A-12'\n",
        "OUTPUT_DIR = f'{repo}/outputs'\n",
        "print(f'***** Model output directory: {OUTPUT_DIR} *****')\n",
        "print(f'***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: model_repo/outputs *****\n",
            "***** BERT pretrained directory: model_repo/uncased_L-12_H-768_A-12 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3a2a0d12de6ecb82812a36549ea910b5a55c0c4a",
        "id": "qGme3LhDokxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "# takes equal amounts of samples from each category\n",
        "# returns train test data set\n",
        "def create_sample(sample_amount):\n",
        "    if sample_amount > 4000:\n",
        "        raise ValueError('cannot sample more then 4000 data points.')\n",
        "    data = pd.read_csv('drive/My Drive/bert_rte/data/wrangled_train.csv', index_col=0)\n",
        "    sampled_text = []\n",
        "    sampled_id = []\n",
        "    sampled_tag = []\n",
        "    for tag in list(data.Tag.unique()):\n",
        "        sampled = data[data['Tag'] == tag].sample(sample_amount, random_state=42)\n",
        "        sampled_text_ = list(sampled['Text'])\n",
        "        sampled_id_ = list(sampled['ID'])\n",
        "        sampled_tag_ = list(sampled['Tag_num'])\n",
        "\n",
        "        for item in sampled_text_:\n",
        "            sampled_text.append(item)\n",
        "        for item in sampled_id_:\n",
        "            sampled_id.append(item)\n",
        "        for item in sampled_tag_:\n",
        "            sampled_tag.append(item)\n",
        "\n",
        "    with open('sampled_data.txt', 'w') as outfile:\n",
        "        for sample_index in range(len(sampled_id)):\n",
        "            json.dump({sampled_id[sample_index]: [sampled_text[sample_index], sampled_tag[sample_index]]}, outfile)\n",
        "            outfile.write('\\n')\n",
        "\n",
        "    with open('sampled_data.txt', 'r') as infile:\n",
        "        data = [json.loads(i) for i in infile if type(list(json.loads(i).values())[0][0]) == str]\n",
        "    X_train, X_test, y_train, y_test = train_test_split([list(i.values())[0][0] for i in data],\n",
        "                                                        [list(i.values())[0][1] for i in data], test_size=0.20,\n",
        "                                                        random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "train_lines, test_lines, train_labels, test_labels = create_sample(2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaQXy-z8NPAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = [i-1 for i in train_labels]\n",
        "test_labels  = [i-1 for i in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b8804d16e48636d8a5c1474c4384076b16c73adc",
        "id": "U7OW5y0zokxm",
        "colab_type": "code",
        "outputId": "51254308-bdd6-47ee-cd14-151fe7f957b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "import modeling\n",
        "import optimization\n",
        "import run_classifier\n",
        "import tokenization\n",
        "import tensorflow as tf\n",
        "\n",
        "# This is the ingestion method, where data is prepared for Bert \n",
        "\n",
        "def create_examples(lines, set_type, labels=None):\n",
        "#Generate data for the BERT model\n",
        "    guid = f'{set_type}'\n",
        "    examples = []\n",
        "    if guid == 'train':\n",
        "        for line, label in zip(lines, labels):\n",
        "            text_a = line\n",
        "            label = str(label)\n",
        "            examples.append(\n",
        "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "    else:\n",
        "        for line in lines:\n",
        "            text_a = line\n",
        "            label = '0'\n",
        "            examples.append(\n",
        "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "    return examples\n",
        "\n",
        "# Model Hyper Parameters\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "WARMUP_PROPORTION = 0.1\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000 #if you wish to finetune a model on a larger dataset, use larger interval\n",
        "# each checpoint weights about 1,5gb\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "NUM_TPU_CORES = 8\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "\n",
        "label_list = [str(i) for i in list(set(train_labels))]\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
        "train_examples = create_examples(train_lines, 'train', labels=train_labels)\n",
        "\n",
        "tpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n",
        "#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "model_fn = run_classifier.model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n",
        "    use_one_hot_embeddings=True)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0706 12:59:29.457189 140026542221184 deprecation_wrapper.py:119] From /content/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0706 12:59:29.461516 140026542221184 deprecation_wrapper.py:119] From /content/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0706 12:59:30.285027 140026542221184 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0706 12:59:30.287150 140026542221184 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5a1ecbba60>) includes params argument, but params are not passed to Estimator.\n",
            "W0706 12:59:30.292601 140026542221184 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b69a1b734026d0b4b16c3eeb405e920195183873",
        "id": "0AVQLCLookxq",
        "colab_type": "code",
        "outputId": "1b06b1a2-3328-4d62-b173-eb8a01117d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "\"\"\"\n",
        "Note: You might see a message 'Running train on CPU'. \n",
        "This really just means that it's running on something other than a Cloud TPU, which includes a GPU.\n",
        "\"\"\"\n",
        "\n",
        "# Train the model.\n",
        "print('Please wait...')\n",
        "train_features = run_classifier.convert_examples_to_features(\n",
        "    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "print('  Num examples = {}'.format(len(train_examples)))\n",
        "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "train_input_fn = run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=True)\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print('***** Finished training at {} *****'.format(datetime.datetime.now()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0706 12:59:30.311134 140026542221184 deprecation_wrapper.py:119] From /content/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Please wait...\n",
            "***** Started training at 2019-07-06 13:01:17.732805 *****\n",
            "  Num examples = 36780\n",
            "  Batch size = 32\n",
            "***** Finished training at 2019-07-06 13:01:17.761552 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "58fb06968c09872ccd74b44e07b4a6e932beb6df",
        "scrolled": true,
        "id": "3wY-dwG-okxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "There is a weird bug in original code.\n",
        "When predicting, estimator returns an empty dict {}, without batch_size.\n",
        "I redefine input_fn_builder and hardcode batch_size, irnoring 'params' for now.\n",
        "\"\"\"\n",
        "\n",
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    print(params)\n",
        "    batch_size = 32\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "20a827baca921e1b48ac3de20422e6d384e2e450",
        "id": "DqOBAvUTokx1",
        "colab_type": "code",
        "outputId": "9a1bbc79-12ae-482d-f565-96ec086cc44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "predict_examples = create_examples(test_lines, 'test', test_labels)\n",
        "\n",
        "predict_features = run_classifier.convert_examples_to_features(\n",
        "    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "predict_input_fn = input_fn_builder(\n",
        "    features=predict_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "\n",
        "result = list(estimator.predict(input_fn=predict_input_fn))\n",
        "predicted_classes = [np.argmax(p['probabilities']) for p in result]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0706 13:01:49.493064 140026542221184 deprecation_wrapper.py:119] From /content/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0706 13:01:49.495870 140026542221184 deprecation_wrapper.py:119] From /content/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0706 13:01:49.537412 140026542221184 deprecation_wrapper.py:119] From /content/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0706 13:01:49.596309 140026542221184 deprecation.py:323] From /content/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0706 13:01:52.124088 140026542221184 deprecation_wrapper.py:119] From /content/run_classifier.py:647: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0706 13:01:52.127489 140026542221184 deprecation_wrapper.py:119] From /content/run_classifier.py:661: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "W0706 13:01:53.251197 140026542221184 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0706 13:01:53.746942 140026542221184 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "4acc876dc974dd4d93525e3b163859ddfda33b2d",
        "id": "YNecUCxUokyE",
        "colab_type": "code",
        "outputId": "5a53464f-8cfa-4e54-d15f-7d94f1fc1813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(predicted_classes, test_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7932796868203567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "49299e2de55fc4136689cc43905fbc4690060c65",
        "id": "adDcqzEwokyI",
        "colab_type": "text"
      },
      "source": [
        "There are several downsides for BERT at this moment:\n",
        "\n",
        "- Training is expensive. All results on the paper were fine-tuned on a single Cloud TPU, which has 64GB of RAM. It is currently not possible to re-produce most of the BERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, because the maximum batch size that can fit in memory is too small. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9A2LvAetuqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_classes = [i+1 for i in predicted_classes]\n",
        "np.save('bert_predictions', predicted_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RkvX_s-S3E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('bert_predictions', predicted_classes)\n",
        "\n",
        "test_labels = [i+1 for i in test_labels]\n",
        "np.save('ground_truth_10', test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}