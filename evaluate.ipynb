{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Ground Truth\n",
    "ground_truth = np.load('predictions/ground_truth.npy')\n",
    "\n",
    "# Transfer Learning Model Predictions\n",
    "bert_pred = np.load('predictions/bert_predictions.npy')\n",
    "glove_pred = np.load('predictions/lstm_glove_predictions.npy')\n",
    "elmo_pred = [np.argmax(i)+1 for i in np.load('predictions/elmo_predict.npy')]\n",
    "\n",
    "# Other Model Predictions\n",
    "svm_pred = [int(i) for i in np.load('predictions/svm_predictions.npy')]\n",
    "log_pred = [int(i) for i in np.load('predictions/log_predictions.npy')]\n",
    "rf_pred  = [int(i) for i in np.load('predictions/rf_predictions.npy')]\n",
    "lstm_pred = np.load('predictions/lstm_predictions.npy')\n",
    "\n",
    "# Small Data Subsample Predictions \n",
    "ground_truth_10 = np.load('predictions/ground_truth_10.npy')\n",
    "bert_pred_10 = np.load('predictions/bert_predictions_10.npy')\n",
    "svm_pred_10 = [int(i) for i in np.load('predictions/svm_predictions_10.npy')]\n",
    "log_pred_10 = [int(i) for i in np.load('predictions/log_predictions_10.npy')]\n",
    "################################################################\n",
    "# Accuracy #\n",
    "bert_acc = round(accuracy_score(ground_truth,bert_pred),2)\n",
    "glove_acc = round(accuracy_score(ground_truth,glove_pred),2)\n",
    "elmo_acc = round(accuracy_score(ground_truth, elmo_pred),2)\n",
    "svm_acc = round(accuracy_score(ground_truth,svm_pred),2)\n",
    "log_acc = round(accuracy_score(ground_truth,log_pred),2)\n",
    "rf_acc = round(accuracy_score(ground_truth,rf_pred),2)\n",
    "lstm_acc = round(accuracy_score(ground_truth,lstm_pred),2)\n",
    "\n",
    "bert_10_acc = round(accuracy_score(ground_truth_10, bert_pred_10),2)\n",
    "svm_10_acc = round(accuracy_score(ground_truth_10, svm_pred_10),2)\n",
    "log_10_acc = round(accuracy_score(ground_truth_10, log_pred_10),2)\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model                     </th><th style=\"text-align: right;\">  Accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>BERT                      </td><td style=\"text-align: right;\">      0.79</td></tr>\n",
       "<tr><td>ELMO                      </td><td style=\"text-align: right;\">      0.73</td></tr>\n",
       "<tr><td>GloVe                     </td><td style=\"text-align: right;\">      0.76</td></tr>\n",
       "<tr><td>SVM TF-IDF                </td><td style=\"text-align: right;\">      0.77</td></tr>\n",
       "<tr><td>Logistic Regression TF-IDF</td><td style=\"text-align: right;\">      0.77</td></tr>\n",
       "<tr><td>Random Forrest TF-IDF     </td><td style=\"text-align: right;\">      0.73</td></tr>\n",
       "<tr><td>LSTM                      </td><td style=\"text-align: right;\">      0.66</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [[\"BERT\", bert_acc], [\"ELMO\", elmo_acc], [\"GloVe\",glove_acc], [\"SVM TF-IDF\",svm_acc], \n",
    "        [\"Logistic Regression TF-IDF\",log_acc], [\"Random Forrest TF-IDF\",rf_acc], [\"LSTM\", lstm_acc]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html', headers=('Model', 'Accuracy'),stralign='left')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_f1 = f1_score(ground_truth,bert_pred, average=None)\n",
    "svm_f1 = f1_score(ground_truth,svm_pred, average=None)\n",
    "log_f1 = f1_score(ground_truth, log_pred, average=None)\n",
    "glove_f1 = f1_score(ground_truth, glove_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Matrix of Classes vs Models where Values are F1-Scores\n",
    "# Generated HTML File\n",
    "\n",
    "import pandas as pd\n",
    "from bokeh.io import output_file, show, save\n",
    "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, ColumnDataSource, PrintfTickFormatter\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import transform\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [bert_f1, svm_f1, log_f1, glove_f1],\n",
    "    index=['BERT', 'SVM-TFIDF', 'LOG-TFIDF', \"GloVe\"],\n",
    "    columns=[str(i) for i in range (1,24)])\n",
    "df.index.name = 'Classes'\n",
    "df.columns.name = 'Models'\n",
    "\n",
    "\n",
    "# Prepare data.frame in the right format\n",
    "df = df.stack().rename(\"value\").reset_index()\n",
    "\n",
    "# here the plot :\n",
    "output_file(\"class_f1scores.html\")\n",
    "\n",
    "# You can use your own palette here\n",
    "colors = ['#ff6775','#ffb3ba','#ffdfba','#ffffba', '#bae1ff', '#baffc9', '#5d7f64']\n",
    "\n",
    "# Had a specific mapper to map color with value\n",
    "mapper = LinearColorMapper(\n",
    "    palette=colors, low=df.value.min(), high=df.value.max())\n",
    "# Define a figure\n",
    "p = figure(\n",
    "    plot_width=1800,\n",
    "    plot_height=900,\n",
    "    title=\"Class F-1 Scores\",\n",
    "    x_range=list(df.Classes.drop_duplicates()),\n",
    "    y_range=list(df.Models.drop_duplicates()),\n",
    "    toolbar_location=None,\n",
    "    tools=\"\",\n",
    "    x_axis_location=\"above\")\n",
    "# Create rectangle for heatmap\n",
    "p.rect(\n",
    "    x=\"Classes\",\n",
    "    y=\"Models\",\n",
    "    width=1,\n",
    "    height=1,\n",
    "    source=ColumnDataSource(df),\n",
    "    line_color='#121914',\n",
    "    fill_color=transform('value', mapper))\n",
    "# Add legend\n",
    "color_bar = ColorBar(\n",
    "    color_mapper=mapper,\n",
    "    location=(0, 0),\n",
    "    ticker=BasicTicker(desired_num_ticks=len(colors)))\n",
    "\n",
    "p.add_layout(color_bar, 'right')\n",
    "tooltips = [('F1','@value'), ('Class', '@Models')]\n",
    "p.add_tools(HoverTool(tooltips=tooltips))\n",
    "save(p)\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
